#!/usr/bin/env python
"""
æ£€æŸ¥ PubMed-BERT æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ä¸”å®Œæ•´
"""
import os
from pathlib import Path

print("=" * 60)
print("ğŸ” æ£€æŸ¥ PubMed-BERT æ¨¡å‹")
print("=" * 60)

# å¯èƒ½çš„ç¼“å­˜è·¯å¾„
possible_paths = [
    "./pretrained_models",
    "./pretrained_models/models--microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext",
    os.path.expanduser("~/.cache/huggingface/hub"),
    os.path.expanduser("~/.cache/huggingface/hub/models--microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"),
]

found_path = None

for path in possible_paths:
    if os.path.exists(path):
        print(f"\nâœ… è·¯å¾„å­˜åœ¨: {path}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
        for root, dirs, files in os.walk(path):
            for f in files:
                if f in ["pytorch_model.bin", "model.safetensors", "config.json"]:
                    full_path = os.path.join(root, f)
                    size_mb = os.path.getsize(full_path) / (1024 * 1024)
                    print(f"   ğŸ“¦ {f}: {size_mb:.1f} MB")
                    
                    if f in ["pytorch_model.bin", "model.safetensors"] and size_mb > 400:
                        found_path = path
    else:
        print(f"âŒ ä¸å­˜åœ¨: {path}")

print("\n" + "=" * 60)

if found_path:
    print("âœ… BERT æ¨¡å‹å·²æ‰¾åˆ°ï¼")
    print(f"ğŸ“ ä½ç½®: {found_path}")
    
    # å°è¯•åŠ è½½
    print("\nğŸ”„ æµ‹è¯•åŠ è½½æ¨¡å‹...")
    try:
        import os
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        from transformers import AutoTokenizer, AutoModel
        
        model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'
        
        # ç¡®å®šæ­£ç¡®çš„ cache_dir
        if "pretrained_models" in found_path:
            cache_dir = "./pretrained_models"
        else:
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        
        print(f"   ä½¿ç”¨ç¼“å­˜ç›®å½•: {cache_dir}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=True
        )
        print("   âœ… Tokenizer åŠ è½½æˆåŠŸ")
        
        model = AutoModel.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=True
        )
        print("   âœ… Model åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç¼–ç 
        import torch
        text = "This is a 25% low-dose CT scan from a GE scanner."
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
        
        with torch.no_grad():
            outputs = model(**inputs)
            cls_emb = outputs.last_hidden_state[:, 0, :]
        
        print(f"\nâœ… æ¨¡å‹åŠŸèƒ½æ­£å¸¸ï¼")
        print(f"   æµ‹è¯•æ–‡æœ¬: {text}")
        print(f"   CLS embedding shape: {cls_emb.shape}")
        print(f"   Embedding norm: {cls_emb.norm().item():.4f}")
        
    except Exception as e:
        print(f"\nâŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
else:
    print("âŒ æœªæ‰¾åˆ° BERT æ¨¡å‹ï¼")
    print("\nè¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
    print("python -c \"from transformers import AutoModel, AutoTokenizer; ")
    print("           AutoModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', cache_dir='./pretrained_models');")
    print("           AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', cache_dir='./pretrained_models')\"")

print("=" * 60)