import torch
import glob
import sys

print("="*70)
print("ğŸš¨ ç´§æ€¥è¯Šæ–­ - PSNR è¿‡ä½")
print("="*70)

# æ‰¾åˆ°æœ€æ–°çš„å®éªŒ
experiments = glob.glob('./output/corediff_*/save_models/model-2500')
if not experiments:
    print("âŒ æ²¡æœ‰æ‰¾åˆ° 2500 æ­¥çš„ checkpoint")
    sys.exit(1)

latest = sorted(experiments)[-1]
exp_name = latest.split('/')[2]
print(f"\nğŸ“ å®éªŒ: {exp_name}")
print(f"ğŸ“ Checkpoint: {latest}\n")

# 1. åŠ è½½ checkpoint
ckpt = torch.load(latest, map_location='cpu')

# 2. æ£€æŸ¥ loss æ˜¯å¦æ­£å¸¸
print("[1/5] æ£€æŸ¥è®­ç»ƒ loss...")
print("-"*70)

# ä»æ—¥å¿—è¯»å–
log_files = glob.glob(f'./output/{exp_name}/logs/*.log')
if log_files:
    with open(log_files[0], 'r') as f:
        lines = f.readlines()[-50:]  # æœ€å50è¡Œ
    
    losses = []
    for line in lines:
        if 'loss' in line.lower() and not 'psnr' in line.lower():
            try:
                # å°è¯•æå– loss å€¼
                parts = line.split(',')
                for part in parts:
                    if 'loss' in part.lower():
                        val = float(part.split()[-1])
                        losses.append(val)
                        break
            except:
                pass
    
    if losses:
        print(f"  æœ€è¿‘çš„ loss å€¼:")
        print(f"    æœ€å°: {min(losses):.6f}")
        print(f"    æœ€å¤§: {max(losses):.6f}")
        print(f"    å¹³å‡: {sum(losses)/len(losses):.6f}")
        print(f"    æœ€å: {losses[-1]:.6f}")
        
        if losses[-1] > 0.05:
            print(f"\n  âŒ Loss å¤ªé«˜ï¼{losses[-1]:.6f} > 0.05")
            print(f"     â†’ æ¨¡å‹æ²¡æœ‰æ”¶æ•›")
        elif losses[-1] < 0.0001:
            print(f"\n  âš ï¸  Loss è¿‡å°ï¼{losses[-1]:.8f} < 0.0001")
            print(f"     â†’ å¯èƒ½è¿‡æ‹Ÿåˆæˆ–æ¢¯åº¦æ¶ˆå¤±")
        else:
            print(f"  âœ… Loss èŒƒå›´æ­£å¸¸")
else:
    print("  âš ï¸  æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")

# 3. æ£€æŸ¥ç½‘ç»œå‚æ•°æ˜¯å¦æ›´æ–°
print("\n[2/5] æ£€æŸ¥ç½‘ç»œå‚æ•°...")
print("-"*70)

param_stats = []
for key, val in ckpt.items():
    if 'weight' in key and 'denoise_fn' in key and 'conv' in key:
        param_stats.append({
            'name': key,
            'max': val.abs().max().item(),
            'mean': val.abs().mean().item()
        })

if param_stats:
    # æŒ‰ max æ’åº
    param_stats.sort(key=lambda x: x['max'], reverse=True)
    
    print(f"  Top 5 å‚æ•°èŒƒå›´:")
    for i, stat in enumerate(param_stats[:5]):
        print(f"    {i+1}. max={stat['max']:.4f}, mean={stat['mean']:.4f}")
    
    max_param = param_stats[0]['max']
    if max_param > 10:
        print(f"\n  âŒ å‚æ•°çˆ†ç‚¸ï¼max={max_param:.2f} > 10")
    elif max_param < 0.001:
        print(f"\n  âŒ å‚æ•°æ¶ˆå¤±ï¼max={max_param:.6f} < 0.001")
    else:
        print(f"\n  âœ… å‚æ•°èŒƒå›´æ­£å¸¸")

# 4. æ£€æŸ¥ FiLM å‚æ•°
print("\n[3/5] æ£€æŸ¥ FiLM å‚æ•°...")
print("-"*70)

film_params = []
for key, val in ckpt.items():
    if 'film' in key.lower() and 'fc.2' in key:  # FiLM è¾“å‡ºå±‚
        film_params.append({
            'name': key,
            'val': val,
            'max': val.abs().max().item(),
            'mean': val.abs().mean().item()
        })

if film_params:
    for fp in film_params:
        print(f"  {fp['name'].split('.')[-3]}:")
        print(f"    max={fp['max']:.6f}, mean={fp['mean']:.6f}")
    
    max_film = max([fp['max'] for fp in film_params])
    if max_film < 0.01:
        print(f"\n  âŒ FiLM å‚æ•°å¤ªå°ï¼max={max_film:.6f} < 0.01")
        print(f"     â†’ FiLM å‡ ä¹ä¸èµ·ä½œç”¨")
    else:
        print(f"\n  âœ… FiLM å‚æ•°æ­£å¸¸")
else:
    print("  âš ï¸  æœªæ‰¾åˆ° FiLM å‚æ•°")

# 5. æµ‹è¯•å®é™…å‰å‘ä¼ æ’­
print("\n[4/5] æµ‹è¯•å‰å‘ä¼ æ’­...")
print("-"*70)

try:
    from models.corediff.corediff_wrapper import Network
    
    # åˆ›å»ºæ–°æ¨¡å‹
    model = Network(in_channels=3, context=True, text_emb_dim=256).cuda()
    
    # å°è¯•åŠ è½½ checkpointï¼ˆåªåŠ è½½åŒ¹é…çš„éƒ¨åˆ†ï¼‰
    model_dict = model.state_dict()
    
    # è¿‡æ»¤å‡ºå¯ä»¥åŠ è½½çš„å‚æ•°
    pretrained_dict = {}
    for k, v in ckpt.items():
        # ç§»é™¤ 'denoise_fn.' å‰ç¼€
        new_k = k.replace('denoise_fn.', '')
        if new_k in model_dict and model_dict[new_k].shape == v.shape:
            pretrained_dict[new_k] = v
    
    print(f"  å¯åŠ è½½å‚æ•°: {len(pretrained_dict)}/{len(model_dict)}")
    
    if len(pretrained_dict) > 0:
        model.load_state_dict(pretrained_dict, strict=False)
        model.eval()
        
        # æµ‹è¯•è¾“å…¥
        x = torch.randn(1, 3, 256, 256).cuda()
        t = torch.tensor([5]).cuda()
        y = torch.randn(1, 1, 256, 256).cuda()
        x_end = torch.randn(1, 1, 256, 256).cuda()
        text_emb = torch.randn(1, 256).cuda()
        
        with torch.no_grad():
            out_no_text, _ = model(x, t, y, x_end, adjust=True, text_emb=None)
            out_with_text, _ = model(x, t, y, x_end, adjust=True, text_emb=text_emb)
        
        diff = (out_with_text - out_no_text).abs().mean().item()
        baseline = out_no_text.abs().mean().item()
        
        print(f"  è¾“å‡ºèŒƒå›´: [{out_no_text.min().item():.4f}, {out_no_text.max().item():.4f}]")
        print(f"  æ–‡æœ¬æ¡ä»¶å½±å“: {diff:.6f}")
        print(f"  ç›¸å¯¹å½±å“: {diff/baseline*100:.4f}%")
        
        if baseline < 0.01 or baseline > 10:
            print(f"\n  âŒ è¾“å‡ºèŒƒå›´å¼‚å¸¸ï¼baseline={baseline:.4f}")
            print(f"     â†’ åº”è¯¥åœ¨ [0, 1] èŒƒå›´å†…")
        else:
            print(f"  âœ… è¾“å‡ºèŒƒå›´æ­£å¸¸")
            
        if diff/baseline < 0.0001:
            print(f"  âŒ æ–‡æœ¬æ¡ä»¶å‡ ä¹æ— å½±å“")
    else:
        print("  âŒ æ— æ³•åŠ è½½ä»»ä½•å‚æ•°ï¼ˆæ¶æ„å®Œå…¨ä¸åŒ¹é…ï¼‰")
        
except Exception as e:
    print(f"  âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# 6. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®
print("\n[5/5] æ£€æŸ¥æ•°æ®åŠ è½½...")
print("-"*70)

try:
    sys.path.append('.')
    from text_conditioned_dataset import TextConditionedCTDataset
    
    dataset = TextConditionedCTDataset(
        dataset='mayo_2016',
        mode='test',
        test_id=9,
        dose=25,
        context=True,
        use_text=True
    )
    
    sample = dataset[0]
    
    print(f"  æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"  è¾“å…¥å½¢çŠ¶: {sample['input'].shape}")
    print(f"  ç›®æ ‡å½¢çŠ¶: {sample['target'].shape}")
    print(f"  è¾“å…¥èŒƒå›´: [{sample['input'].min():.4f}, {sample['input'].max():.4f}]")
    print(f"  ç›®æ ‡èŒƒå›´: [{sample['target'].min():.4f}, {sample['target'].max():.4f}]")
    
    if sample['input'].max() > 2 or sample['input'].min() < -1:
        print(f"\n  âŒ æ•°æ®æœªå½’ä¸€åŒ–ï¼èŒƒå›´åº”è¯¥åœ¨ [0, 1]")
    else:
        print(f"  âœ… æ•°æ®èŒƒå›´æ­£å¸¸")
        
except Exception as e:
    print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")

# ç»ˆæè¯Šæ–­
print("\n" + "="*70)
print("ğŸ¯ å¯èƒ½çš„é—®é¢˜:")
print("="*70)

print("""
æ ¹æ®ä¸Šè¿°è¯Šæ–­ï¼Œæœ€å¯èƒ½çš„é—®é¢˜æ˜¯ï¼š

1. å­¦ä¹ ç‡é—®é¢˜
   - å¦‚æœ loss ä¸ä¸‹é™ â†’ å­¦ä¹ ç‡å¤ªå°
   - å¦‚æœ loss éœ‡è¡æˆ– NaN â†’ å­¦ä¹ ç‡å¤ªå¤§

2. æ•°æ®é—®é¢˜
   - å¦‚æœè¾“å…¥/è¾“å‡ºèŒƒå›´å¼‚å¸¸ â†’ æ•°æ®é¢„å¤„ç†é”™è¯¯
   - å¦‚æœæ•°æ®é›†å¾ˆå° â†’ å¯èƒ½è¿‡æ‹Ÿåˆ

3. FiLM é—®é¢˜
   - å¦‚æœ FiLM å‚æ•°å¤ªå° â†’ æ²¡è¢«è®­ç»ƒ
   - å¦‚æœæ–‡æœ¬æ¡ä»¶æ— å½±å“ â†’ FiLM åˆå§‹åŒ–æœ‰é—®é¢˜

4. æ¶æ„é—®é¢˜
   - å¦‚æœæ— æ³•åŠ è½½å‚æ•° â†’ ä»£ç å’Œ checkpoint ä¸åŒ¹é…
   - å¦‚æœè¾“å‡ºèŒƒå›´å¼‚å¸¸ â†’ æ¨¡å‹è¾“å‡ºå±‚æœ‰é—®é¢˜

å»ºè®®ä¿®å¤é¡ºåºï¼š
  â†’ å…ˆä¸ç”¨ text conditionï¼Œè·‘çº¯ baseline
  â†’ å¦‚æœ baseline æ­£å¸¸ï¼Œå†åŠ  text condition
  â†’ å¦‚æœ baseline ä¹Ÿä¸è¡Œï¼Œæ£€æŸ¥æ•°æ®å’Œä»£ç 
""")

print("\n" + "="*70)
print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
print("="*70)
print("1. æŸ¥çœ‹ä¸Šè¿°è¯Šæ–­ç»“æœ")
print("2. å¦‚æœæ˜¯æ•°æ®/æ¶æ„é—®é¢˜ï¼Œå…ˆä¿®å¤")
print("3. å¦‚æœåªæ˜¯ FiLM é—®é¢˜ï¼Œå¯ä»¥å…ˆè·‘ baseline")
print("\nè¿è¡Œ baseline æµ‹è¯•:")
print("  bash train_baseline_test.sh")