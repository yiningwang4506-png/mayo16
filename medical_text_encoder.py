"""
Medical Text Encoder using PubMed-BERT
ä½¿ç”¨ PubMed-BERT å°†åŒ»å­¦æè¿°æ–‡æœ¬ç¼–ç ä¸ºæ¡ä»¶å‘é‡
ä¼˜åŒ–ç‰ˆï¼šå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å… Hugging Face ç½‘ç»œè¶…æ—¶
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import os


class MedicalTextEncoder(nn.Module):
    """
    åŒ»å­¦æ–‡æœ¬ç¼–ç å™¨
    ä½¿ç”¨é¢„è®­ç»ƒçš„ PubMed-BERT / BioLinkBERT å°†åŒ»å­¦æè¿°ç¼–ç ä¸ºæ¡ä»¶å‘é‡
    """
    
    def __init__(self, 
                 model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',
                 output_dim=256,
                 freeze_bert=True,
                 cache_dir='./pretrained_models',
                 local_files_only=True):  # â† æ–°å¢å‚æ•°ï¼Œé»˜è®¤True
        """
        Args:
            model_name: Hugging Face æ¨¡å‹åç§°
                - 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' (æ¨è)
                - 'michiyasunaga/BioLinkBERT-base'
                - 'dmis-lab/biobert-base-cased-v1.1'
            output_dim: è¾“å‡ºembeddingç»´åº¦ (é»˜è®¤256,ä¸CoreDiffå…¼å®¹)
            freeze_bert: æ˜¯å¦å†»ç»“BERTå‚æ•° (æ¨èTrueä»¥èŠ‚çœæ˜¾å­˜)
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            local_files_only: æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼Œé¿å…ç½‘ç»œè¶…æ—¶ï¼‰
        """
        super().__init__()
        
        self.model_name = model_name
        self.output_dim = output_dim
        
        # ============ å…³é”®ä¿®æ”¹ï¼šè®¾ç½®ç¦»çº¿æ¨¡å¼ ============
        if local_files_only:
            print("ğŸ“¦ Using LOCAL CACHE ONLY (offline mode)")
            # åŒé‡ä¿é™©ï¼šè®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            os.environ['HF_HUB_OFFLINE'] = '1'
        
        # åŠ è½½é¢„è®­ç»ƒtokenizerå’Œæ¨¡å‹
        print(f"ğŸ”„ Loading medical text encoder: {model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                cache_dir=cache_dir,
                local_files_only=local_files_only  # â† å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°
            )
            self.bert = AutoModel.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                local_files_only=local_files_only  # â† å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°
            )
            print("âœ… Loaded from local cache successfully!")
            
        except Exception as e:
            print(f"âŒ Failed to load from local cache!")
            print(f"   Error: {e}")
            print(f"\nğŸ’¡ Please download the model first:")
            print(f"   python medical_text_encoder.py --download")
            raise
        
        # å†»ç»“BERTå‚æ•°
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
            print("  âœ“ BERT parameters frozen")
        
        # BERTè¾“å‡ºç»´åº¦ (é€šå¸¸æ˜¯768)
        bert_dim = self.bert.config.hidden_size
        
        # æŠ•å½±å±‚: 768D â†’ output_dim
        self.projection = nn.Sequential(
            nn.Linear(bert_dim, output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        print(f"  âœ“ Text encoder initialized: {bert_dim}D â†’ {output_dim}D")
    
    def forward(self, text_descriptions):
        """
        å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        
        Args:
            text_descriptions: List[str] or str - åŒ»å­¦æè¿°å¥å­
            
        Returns:
            torch.Tensor: [batch_size, output_dim] - æ–‡æœ¬embedding
        """
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        if isinstance(text_descriptions, str):
            text_descriptions = [text_descriptions]
        
        # Tokenize
        inputs = self.tokenizer(
            text_descriptions,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=128  # CTæè¿°é€šå¸¸ä¸è¶…è¿‡128 tokens
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        device = next(self.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # BERTç¼–ç 
        with torch.set_grad_enabled(not self.bert.training):
            outputs = self.bert(**inputs)
        
        # ä½¿ç”¨ [CLS] token çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [B, 768]
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        text_embedding = self.projection(cls_embedding)  # [B, output_dim]
        
        return text_embedding
    
    def encode(self, text_descriptions):
        """
        ä¾¿æ·ç¼–ç æ–¹æ³•ï¼ˆDatasetæ¨ç†ä¸“ç”¨ï¼‰
        åœ¨æ¨ç†æ—¶è‡ªåŠ¨å¤„ç†ç»´åº¦ï¼Œé€‚åˆåœ¨Datasetä¸­ä½¿ç”¨
        
        Args:
            text_descriptions: str or List[str] - åŒ»å­¦æè¿°
            
        Returns:
            torch.Tensor: 
                - å¦‚æœè¾“å…¥æ˜¯ str: è¿”å› [output_dim]
                - å¦‚æœè¾“å…¥æ˜¯ List[str]: è¿”å› [batch_size, output_dim]
        """
        was_training = self.training
        self.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        
        with torch.no_grad():
            embeddings = self.forward(text_descriptions)
            
            # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œå»æ‰batchç»´åº¦
            if isinstance(text_descriptions, str):
                embeddings = embeddings.squeeze(0)  # [output_dim]
        
        # æ¢å¤åŸæ¥çš„æ¨¡å¼
        if was_training:
            self.train()
        
        return embeddings
    
    def encode_batch(self, dose_list, site_list, **kwargs):
        """
        ä¾¿æ·æ–¹æ³•ï¼šç›´æ¥ä»å…ƒæ•°æ®ç¼–ç 
        
        Args:
            dose_list: List[int] - å‰‚é‡åˆ—è¡¨
            site_list: List[str] - ç«™ç‚¹åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™ TextDescriptionGenerator
            
        Returns:
            torch.Tensor: [batch_size, output_dim]
        """
        from text_description_generator import TextDescriptionGenerator
        
        # ç”Ÿæˆæè¿°
        generator = TextDescriptionGenerator()
        descriptions = [
            generator.generate_description(dose=dose, site=site, **kwargs)
            for dose, site in zip(dose_list, site_list)
        ]
        
        # ç¼–ç 
        return self.encode(descriptions)


class CachedTextEncoder(MedicalTextEncoder):
    """
    å¸¦ç¼“å­˜çš„æ–‡æœ¬ç¼–ç å™¨
    å¯¹äºè®­ç»ƒé›†ä¸­çš„å›ºå®šæè¿°,ç¼“å­˜embeddingä»¥åŠ é€Ÿè®­ç»ƒ
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cache = {}  # {description: embedding}
        self.use_cache = True
    
    def forward(self, text_descriptions):
        """
        å¸¦ç¼“å­˜çš„å‰å‘ä¼ æ’­
        """
        if not self.use_cache:
            return super().forward(text_descriptions)
        
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        if isinstance(text_descriptions, str):
            text_descriptions = [text_descriptions]
        
        # æ£€æŸ¥ç¼“å­˜
        cached_embeddings = []
        uncached_descriptions = []
        uncached_indices = []
        
        for i, desc in enumerate(text_descriptions):
            if desc in self.cache:
                cached_embeddings.append(self.cache[desc])
            else:
                uncached_descriptions.append(desc)
                uncached_indices.append(i)
        
        # å¦‚æœå…¨éƒ¨å‘½ä¸­ç¼“å­˜
        if len(uncached_descriptions) == 0:
            return torch.stack(cached_embeddings)
        
        # ç¼–ç æœªç¼“å­˜çš„æè¿°
        new_embeddings = super().forward(uncached_descriptions)
        
        # æ›´æ–°ç¼“å­˜
        for desc, emb in zip(uncached_descriptions, new_embeddings):
            self.cache[desc] = emb.detach()
        
        # åˆå¹¶ç»“æœ
        all_embeddings = []
        cached_idx = 0
        uncached_idx = 0
        
        for i in range(len(text_descriptions)):
            if i in uncached_indices:
                all_embeddings.append(new_embeddings[uncached_idx])
                uncached_idx += 1
            else:
                all_embeddings.append(cached_embeddings[cached_idx])
                cached_idx += 1
        
        return torch.stack(all_embeddings)
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        print(f"âœ“ Cache cleared")
    
    def save_cache(self, path):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        torch.save(self.cache, path)
        print(f"âœ“ Cache saved to {path} ({len(self.cache)} entries)")
    
    def load_cache(self, path):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        self.cache = torch.load(path)
        print(f"âœ“ Cache loaded from {path} ({len(self.cache)} entries)")


# ============== å·¥å…·å‡½æ•°ï¼šé¦–æ¬¡ä¸‹è½½æ¨¡å‹ ==============
def download_model_if_needed(model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',
                             cache_dir='./pretrained_models'):
    """
    é¦–æ¬¡ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
    è¿è¡Œä¸€æ¬¡åï¼Œåç»­å¯ä»¥ç¦»çº¿ä½¿ç”¨
    
    Usage:
        python medical_text_encoder.py --download
    """
    print(f"ğŸ“¥ Downloading model: {model_name}")
    print(f"ğŸ“ Cache directory: {cache_dir}")
    print(f"â³ This may take a few minutes...")
    
    try:
        # ä¸´æ—¶å…è®¸ç½‘ç»œè®¿é—®
        print("\nğŸ”„ Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=False  # å…è®¸ä¸‹è½½
        )
        
        print("ğŸ”„ Downloading model...")
        model = AutoModel.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=False  # å…è®¸ä¸‹è½½
        )
        
        print("\nâœ… Model downloaded successfully!")
        print(f"   Tokenizer vocab size: {tokenizer.vocab_size}")
        print(f"   Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
        print(f"   Cache location: {cache_dir}")
        return True
        
    except Exception as e:
        print(f"\nâŒ Download failed: {e}")
        print(f"ğŸ’¡ Please check your network connection and try again")
        return False


# ============== ç¤ºä¾‹ç”¨æ³• ==============
if __name__ == '__main__':
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹è½½æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--download':
        print("\n" + "="*60)
        print("ğŸš€ Model Download Mode")
        print("="*60 + "\n")
        success = download_model_if_needed()
        sys.exit(0 if success else 1)
    
    # æ­£å¸¸æµ‹è¯•æ¨¡å¼
    print("\n" + "="*60)
    print("ğŸ§ª Testing MedicalTextEncoder (Offline Mode)")
    print("="*60 + "\n")
    
    try:
        # åˆ›å»ºç¼–ç å™¨ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
        encoder = MedicalTextEncoder(
            output_dim=256,
            freeze_bert=True,
            local_files_only=True  # å¼ºåˆ¶ç¦»çº¿
        )
        encoder.eval()
        
        # ç¤ºä¾‹1: å•ä¸ªæè¿°ï¼ˆä½¿ç”¨ encode æ–¹æ³•ï¼‰
        print("\nğŸ“ Test 1: Single Description (encode method)")
        desc = "This CT scan was acquired using a 25% low-dose protocol. The image comes from Mayo Clinic 2016 dataset."
        embedding = encoder.encode(desc)
        print(f"âœ… Single description encoding: {embedding.shape}")  # [256]
        print(f"   Embedding norm: {embedding.norm().item():.4f}")
        
        # ç¤ºä¾‹2: æ‰¹é‡æè¿°ï¼ˆä½¿ç”¨ encode æ–¹æ³•ï¼‰
        print("\nğŸ“ Test 2: Batch Descriptions (encode method)")
        descs = [
            "This CT scan was acquired using a 25% low-dose protocol.",
            "This CT scan was acquired using a 10% ultra-low-dose protocol.",
            "This CT scan was acquired using a full-dose protocol."
        ]
        embeddings = encoder.encode(descs)
        print(f"âœ… Batch description encoding: {embeddings.shape}")  # [3, 256]
        
        # ç¤ºä¾‹3: forward æ–¹æ³•ï¼ˆè®­ç»ƒæ—¶ï¼‰
        print("\nğŸ“ Test 3: Forward Method (for training)")
        with torch.no_grad():
            embeddings_forward = encoder.forward(descs)
        print(f"âœ… Forward method output: {embeddings_forward.shape}")  # [3, 256]
        
        # ç¤ºä¾‹4: ä»å…ƒæ•°æ®ç›´æ¥ç¼–ç 
        print("\nğŸ“ Test 4: Encoding from Metadata")
        embeddings = encoder.encode_batch(
            dose_list=[25, 25, 10],
            site_list=['mayo_2016', 'mayo_2020', 'mayo_2016']
        )
        print(f"âœ… Encoding from metadata: {embeddings.shape}")  # [3, 256]
        
        # ç¤ºä¾‹5: éªŒè¯embeddingçš„è¯­ä¹‰ç›¸ä¼¼æ€§
        print("\nğŸ“ Test 5: Semantic Similarity")
        with torch.no_grad():
            emb_25 = encoder.encode("25% low-dose protocol with increased noise")
            emb_10 = encoder.encode("10% ultra-low-dose protocol with high noise")
            emb_full = encoder.encode("full-dose protocol with standard quality")
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            sim_25_10 = torch.nn.functional.cosine_similarity(
                emb_25.unsqueeze(0), emb_10.unsqueeze(0)
            ).item()
            sim_25_full = torch.nn.functional.cosine_similarity(
                emb_25.unsqueeze(0), emb_full.unsqueeze(0)
            ).item()
            
            print(f"  25% vs 10% dose: {sim_25_10:.4f} (should be high)")
            print(f"  25% vs full dose: {sim_25_full:.4f} (should be lower)")
        
        # ç¤ºä¾‹6: æµ‹è¯•ç»´åº¦å¤„ç†
        print("\nğŸ“ Test 6: Dimension Handling")
        single_emb = encoder.encode("Single description")
        batch_emb = encoder.encode(["Batch description"])
        print(f"  Single string â†’ shape: {single_emb.shape}")  # [256]
        print(f"  List with 1 item â†’ shape: {batch_emb.shape}")  # [1, 256]
        
        print("\n" + "="*60)
        print("âœ… All tests passed!")
        print("="*60)
        print("\nğŸ’¡ Usage in your code:")
        print("   text_embedding = encoder.encode(description)  # Returns [256]")
        print("   text_embeddings = encoder.encode([desc1, desc2])  # Returns [2, 256]")
        
    except Exception as e:
        print("\n" + "="*60)
        print("âŒ Test failed!")
        print("="*60)
        print(f"\nError: {e}")
        print("\nğŸ’¡ If model not found, please run:")
        print("   python medical_text_encoder.py --download")
        import traceback
        traceback.print_exc()
        sys.exit(1)